\documentclass{article}
\usepackage[utf8]{inputenc}
\usepackage[a4paper, total={6in, 10in}]{geometry}
\usepackage{amsmath}
\usepackage{amsthm}
\usepackage{amsfonts}
\usepackage{amssymb}
\usepackage{tikz}

\def\argmax{\mathop{\rm arg\,max}}
\def\argmin{\mathop{\rm arg\,min}}
\def\norm#1{\|#1\|}
\newcommand{\ds}{\displaystyle}
\newcommand{\set}[1]{\left\{ #1 \right\}}   % Set
\newcommand{\N}{\mathbb{N}}                 % Set of naturals
\newcommand{\Z}{\mathbb{Z}}                 % Set of integers
\newcommand{\Q}{\mathbb{Q}}                 % Set of rationals
\newcommand{\R}{\mathbb{R}}                 % Set of real numbers
\newcommand{\C}{\mathbb{C}}                 % Set of complex numbers
\newcommand{\inn}[1]{\langle {#1} \rangle}  % Vectors
\newcommand{\Arg}{\text{Arg}}
\newcommand{\re}{\text{Re}}
\newcommand{\im}{\text{Im}}
\newcommand{\Log}{\text{Log}}

\title{Neural Networks for Multi-Class Classification}
\author{Joey Shi}
\date{July 2020}

\begin{document}

\maketitle

\section{Abstract}
In this paper, we will be formalizing the process of training a neural network classification model. We will be covering the softmax loss function, computing the gradient of the network, and gradient descent.

\vspace{1cm}

\section{Notation}
\begin{enumerate}
\item $X$ is a $n \times d$ feature matrix of real numbers, such that each row $x_i\in\R^d$ is an example.
\item $y$ is an $n$-dimensional label vector, such that each $y_i\in\set{1,2,\dots , k_L}$ is a class label.
\item $h(z)$ is a non-linear function. We will call it an activation function.
\item $W^{(l)}$ is a $k_{l} \times k_{l-1}$ weight matrix.
\item $b^{(l)}$ is a $k_{l}$-dimensional bias vector.
\item $p(y_i \mid D, x_i)$ is the probability of predicting $y_i$ from $x_i$ and parameters $D$
\item $\delta_{a, b} = 1$ if $a = b$, $\delta_{a, b} = 0$ otherwise [Kronecker delta function].
\end{enumerate}

\vspace{1cm}

% \section{Feed Forwards}
% Consider a training example $x_i$. We begin by lowering the dimension of our training example with a nonlinear change of basis $T_1: \R^{d} \xrightarrow{} \R^{k_1}$, $k_1 << d$. Define $T_1(x_i) = h(W^{(1)}x_i + b^{(1)})$. By making this change of basis $T_1(x_i) = a_i^{(1)}$, we can find latent factors in our data. We repeat this process for each layer. Then, our prediction is  
% \vspace{1cm}

\section{Maximum Likelihood Estimate}
We begin by defining our model's probability mass distribution:
\begin{align*}
    p(y_i \mid D, x_i) &= \frac{\exp\left(z_{i,y_i}^{(L)}\right)}{\sum_{c=1}^k \exp\left(z_{i,c}^{(L)}\right)}, &\left[z_i^{(L)} \text{ is defined in section 4}\right] 
\end{align*}
We choose this probability mass function for our classification problem because of its behaviour: $p(y_i \mid D, x_i)$ is larger if $z_{i,y_i}^{(L)} = \max_{c} z_{i,c}^{(L)}$ and small otherwise. For our model, we want to maximize this probability across all training example, so we maximize $p(y \mid D, X)$. Assuming that each example is i.i.d., we can equivalently minimize $-\log(p(y \mid D, X))$ to avoid underflow during computation.
\begin{align*}    
    -\log(p(y \mid D, X)) &= -\log\left(\prod_{i=1}^n p(y_i = c \mid D, x_i)\right) \\
    &= \sum_{i=1}^n -\log(p(y_i \mid D, x_i)) \\
    &= \sum_{i=1}^n -z_{i, y_i}^{(L)} + \log\left(\sum_{c=1}^{k} \exp\left(z_{i, c}^{(L)}\right)\right) &[\text{Softmax Loss Function}]
\end{align*}



\newpage

\section{Objective Function}
Consider the following feed-forward neural network. For an input layer defined by $x_i$, our hidden layers are defined by the activation vectors $a_i^{(l)}$ and our output layer is defined by $z_i^{(L)}$. \vspace{1mm}

\noindent Note: We add hidden layers so our model can learn about latent factors in the data during training.
\begin{align*}
    a_i^{(0)} &= x_i & z_i^{(1)} &= W^{(1)}a_i^{(0)} + b^{(1)} \\
    a_i^{(1)} &= h\left(z^{(1)}\right) & z_i^{(2)} &= W^{(2)}a_i^{(1)} + b^{(2)} \\
    \vdots & & \vdots \\
    a_i^{(L-1)} &= h\left(z^{(L-1)}\right) & z_i^{(L)} &= W^{(L)}a_i^{(L-1)} + b^{(L)} 
\end{align*}
After some weight initialization, given an example $x_i$, our model would predict $\displaystyle\hat{y}_i = \argmax_{c} z_{i,c}^{(L)}$. Using softmax loss, our objective function becomes
\begin{align*}
    f(W^{(1)},b^{(1)},\dots,W^{(L)},b^{(L)}) &= \sum_{i=1}^{n} \left[-z_{i, y_i}^{(L)} + \log\left(\sum_{c=1}^{k} \exp\left(z_{i, c}^{(L)}\right)\right)\right]
\end{align*}

% \tikzset{
%   every neuron/.style={
%     circle,
%     draw,
%     minimum size=1cm
%   },
%   neuron missing/.style={
%     draw=none, 
%     scale=4,
%     text height=0.3cm,
%     execute at begin node=\color{black}$\vdots$
%   },
% }

% \begin{figure}
% \centering
    
% \begin{tikzpicture}[x=1.5cm, y=1.5cm, >=stealth]

% \foreach \m [count=\y] in {1,2,3,missing,4}
%   \node [every neuron/.try, neuron \m/.try] (input-\m) at (0,2.5-\y) {};

% \foreach \m [count=\y] in {1,missing,2}
%   \node [every neuron/.try, neuron \m/.try ] (hidden-\m) at (2,2-\y*1.25) {};

% \foreach \m [count=\y] in {1,missing,2}
%   \node [every neuron/.try, neuron \m/.try ] (output-\m) at (4,1.5-\y) {};

% \foreach \l [count=\i] in {1,2,3,n}
%   \draw [<-] (input-\i) -- ++(-1,0)
%     node [above, midway] {$x_{i, \l}$};

% \foreach \l [count=\i] in {1,n}
%   \node [above] at (hidden-\i.north) {$a_{i, \l}^{(1)}$};

% \foreach \l [count=\i] in {1,n}
%   \draw [->] (output-\i) -- ++(1,0)
%     node [above, midway] {$z_{i, \l}^{(2)}$};

% \foreach \i in {1,...,4}
%   \foreach \j in {1,...,2}
%     \draw [->] (input-\i) -- (hidden-\j);

% \foreach \i in {1,...,2}
%   \foreach \j in {1,...,2}
%     \draw [->] (hidden-\i) -- (output-\j);

% \foreach \l [count=\x from 0] in {Input, Hidden, Output}
%   \node [align=center, above] at (\x*2,2) {\l \\ layer};

% \end{tikzpicture}
% \end{figure}
\vspace{1cm}

\section{Gradients of the Objective Function}
\newtheorem{theorem}{Theorem}
\begin{theorem}
Let $A^{(l)}$ be a matrix such that each row is an activation vector $a_i^{(l)}$ and let $Z^{(l)}$ be defined similarly. Define a new matrix R by the recursion relation $R^{(l-1)} = R^{(l)}W^{(l)} \circ h\,'\left(Z^{(l-1)}\right)$ with a base case of $R^{(L)}$, where $r_{i, c}^{(L)} = p(y_i = c \mid D, x_i) - \delta_{y_i, c}$. Then the derivatives of the weights and biases of our network are given by \[\frac{\partial f}{\partial W^{(l)}} = \left(R^{(l)}\right)^TA^{(l-1)} \hspace{1cm} \frac{\partial f}{\partial b^{(l)}} = \sum_{i=1}^{n} r_i^{(l)} \]
\end{theorem}

\vspace{1cm}

\textbf{Gradient of $W^{(L)}$ and $b^{(L)}$}
\begin{align*}
    \frac{\partial f}{\partial w_{c,j}^{(L)}} &= \sum_{i=1}^{n} \left[-a_{i,j}^{(L-1)}\delta_{y_i, c} + \frac{1}{\sum_{c_0=1}^{k_l} \exp\left(z_{i,c_0}^{(L)}\right)}  \cdot \exp\left(z_{i,c}^{(L)}\right) \cdot a_{i,j}^{(L-1)}\right] &[\text{Chain Rule}] \\
    &= \sum_{i=1}^{n} [p(y_i = c \mid D, x_i) - \delta_{y_i,c}]a_{i,j}^{(L-1)} \\
    &= \sum_{i=1}^{n} r_{i,c}^{(L)}a_{i,j}^{(L-1)} \\
    \frac{\partial f}{\partial W^{(L)}} &= \left(R^{(L)}\right)^TA^{(L-1)} \\
    \frac{\partial f}{\partial b_{c}^{(L)}} &= \sum_{i=1}^{n} r_{i, c}^{(L)} \cdot 1 &[\text{Similar work as above}] \\
    \frac{\partial f}{\partial b^{(L)}} &= \sum_{i=1}^{n} r_i^{(L)} 
\end{align*}

\newpage

\textbf{Gradient of $W^{(L-1)}$ and $b^{(L-1)}$}
\begin{align*}
    \frac{\partial f}{\partial w_{j, k}^{(L-1)}} &= \sum_{i=1}^{n} \sum_{c=1}^{k_L} \left[p(y_i = c \mid D, x_i) - \delta_{y_i, c}\right] \cdot w_{c,j}^{(L)}  h\,'\left(z_{j, k}^{(L-1)}\right) \cdot a_{j, k}^{(L-2)} &[\text{Chain Rule}]\\
    &= \sum_{i=1}^{n} \sum_{c=1}^{k_L} r_{i,c}^{(L)} \cdot w_{c,k}^{(L)}h\,'\left(z_{i, k}^{(L-1)}\right) \cdot a_{i, k}^{(L-2)} \\
    &= \sum_{i=1}^{n} \left[\left(W^{(L)}\right)^T r_i^{(L)} \circ h\,'\left(z_{i}^{(L-1)}\right)\right] \cdot a_i^{(L-2)} \\
    &= \sum_{i=1}^{n} r_{i}^{(L-1)} \cdot a_i^{(L-2)} \\
    &= \left(R^{(L-1)}\right)^T A^{(L-2)} \\
    \frac{\partial f}{\partial b_{k}^{(L-1)}} &= \sum_{i=1}^{n} r_{i, k}^{(L-1)} \cdot 1 &[\text{Similar work as above}] \\
    \frac{\partial f}{\partial b^{(L-1)}} &= \sum_{i=1}^{n} r_i^{(L-1)} 
\end{align*}

\vspace{1cm}

\textbf{The following derivations are trivial and left as an easy exercise for the reader}

\vspace{1cm}

\section{Stochastic Gradient Descent Algorithm}
Define $D$ to be a vector containing all the weights and biases in our model. Then $f(D)$ is our loss function. Given $E$ epoches, $B$ number of batches, and a learning rate $\alpha^t$, we proceed with minimizing $f(D)$ by stochastic gradient descent. Randomly initialize $D^0$. For each epoch, for each batch, sample $\lfloor n / B \rfloor$ training examples without replacement and compute $\nabla f(D^{t})$. Then,
\begin{align*}
    D^{t+1} &= D^{t} - \alpha^t \nabla f(D^{t}) 
\end{align*}
We will use the parameters $D^E$ to make predictions for the model. 

\vspace{1cm}

\section{Using the Model}
Load a dataset $X$ and $y$, shuffle the examples, and split the dataset into a training set $X_{\text{train}}$ and $y_{\text{train}}$ and testing set $X_{\text{test}}$ and $y_{\text{test}}$. Instantiate our model with a specific number of hidden units for each hidden layer. Fit our model over $X_{\text{train}}$ and $y_{\text{train}}$. Compute training and testing error to evaluate the quality of our model. We can then pass in our own example for our model to predict. Pseudocode is provided below. 
\vspace{1mm}

\noindent Golden Rule: The testing set must not affect the training phase in any way.

\begin{verbatim}
    X, y = load(dataset)
    X_train, y_train, X_test, y_test = shuffle_and_split(X, y)
    model = NNClassifier(hidden_layers)
    model.fit(X_train, y_train)
    
    training_error = count(model.predict(X_train) != y_train) / n_train 
    testing_error = count(model.predict(X_test) != y_test) / n_test
    
    for any custom example x, we can predict y_hat = model.predict(x) 
\end{verbatim}

\end{document}
